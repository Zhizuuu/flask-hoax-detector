{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64-zRexJuN5G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_excel('/content/datafix.xlsx')\n",
        "\n",
        "# Print the dataframe\n",
        "print(df)\n",
        "\n",
        "# Display the distribution of the 'Label' column (0s and 1s)\n",
        "label_distribution = df['Label'].value_counts()\n",
        "print(\"\\nDistribusi jumlah data pada kolom Label:\")\n",
        "print(label_distribution)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define stopwords for Indonesian\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to preprocess text with lemmatization and without tokenization\n",
        "def preprocess_text(text):\n",
        "    # 1. Remove unwanted characters (Regex)\n",
        "    text = re.sub(r'\\W', ' ', str(text))  # Keep only alphabetic characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Replace multiple spaces with a single space\n",
        "\n",
        "    # 2. Case Folding: Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 3. Remove stopwords (Filtering)\n",
        "    filtered_words = [word for word in text.split() if word not in stop_words]\n",
        "\n",
        "    # 4. Lemmatization (Apply lemmatizer)\n",
        "    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in filtered_words])\n",
        "\n",
        "    # 5. Return the lemmatized and cleaned text\n",
        "    return lemmatized_text\n",
        "\n",
        "# Apply preprocessing to the 'Judul' and 'Konten' columns\n",
        "X = df[['Judul', 'Konten']]  # Feature columns (both 'Judul' and 'Konten')\n",
        "y = df['Label']  # Target column ('Label')\n",
        "\n",
        "# Preprocess the 'Judul' and 'Konten' columns\n",
        "X_preprocessed = X.apply(lambda col: col.apply(preprocess_text))\n",
        "\n",
        "# Add the preprocessed columns to the DataFrame\n",
        "df['Judul_preprocessed'] = X_preprocessed['Judul']\n",
        "df['Konten_preprocessed'] = X_preprocessed['Konten']\n",
        "\n",
        "# Show the first few rows of the DataFrame with preprocessed columns\n",
        "print(\"\\nPreview of DataFrame with Preprocessed Columns:\")\n",
        "print(df[['Judul', 'Judul_preprocessed', 'Konten', 'Konten_preprocessed']].head())\n"
      ],
      "metadata": {
        "id": "h9HWzeugyOgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelling**"
      ],
      "metadata": {
        "id": "eEEODdwZ11Cg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBoost**"
      ],
      "metadata": {
        "id": "85Ix3AaJTcGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Import TfidfVectorizer for feature extraction\n",
        "from stopwords import get_stopwords  # Library for stopwords\n",
        "\n",
        "# Get the Indonesian stopwords from the stopwords library\n",
        "indonesian_stopwords = get_stopwords('id')\n",
        "\n",
        "# Assuming df already contains preprocessed 'Judul_preprocessed' and 'Konten_preprocessed' columns as raw text\n",
        "df['text'] = df['Judul_preprocessed'] + \" \" + df['Konten_preprocessed']\n",
        "\n",
        "# Tokenize and remove Indonesian stopwords using the stopwords list\n",
        "def tokenize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in indonesian_stopwords]\n",
        "    return tokens\n",
        "\n",
        "# Apply the tokenizer to the 'text' column\n",
        "df['tokens'] = df['text'].apply(tokenize_text)\n",
        "\n",
        "# Join the tokenized text back into a string for TF-IDF\n",
        "df['text_joined'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Splitting data into 60% train, 20% test, and 20% validation sets\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['text_joined'], df['Label'], test_size=0.4, random_state=42)\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=indonesian_stopwords, max_features=5000)\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train = tfidf_vectorizer.fit_transform(train_texts).toarray()  # Convert to array (dense matrix)\n",
        "\n",
        "# Transform the validation and test data\n",
        "X_val = tfidf_vectorizer.transform(val_texts).toarray()\n",
        "X_test = tfidf_vectorizer.transform(test_texts).toarray()\n",
        "\n",
        "# Reshape the data to be suitable for LSTM (LSTM expects 3D input)\n",
        "# Reshape to (samples, timesteps, features)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_val = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2]), dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))  # For binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, train_labels, epochs=5, batch_size=64, validation_data=(X_val, val_labels), verbose=2)\n",
        "\n",
        "# Evaluate the model on the validation dataset\n",
        "val_predictions = model.predict(X_val)\n",
        "\n",
        "# Flatten the predictions to 1D array for comparison\n",
        "val_predictions = val_predictions.flatten()  # Convert from (n_samples, 1) to (n_samples,)\n",
        "\n",
        "# Convert predictions to binary labels (True or False) based on threshold 0.5\n",
        "val_predictions = (val_predictions > 0.5)\n",
        "\n",
        "# Calculate accuracy for validation dataset\n",
        "val_acc = np.mean(val_predictions == val_labels)\n",
        "print(f\"\\nValidation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "# Calculate the confusion matrix for validation data\n",
        "val_cm = confusion_matrix(val_labels, val_predictions)\n",
        "\n",
        "# Plot the confusion matrix for validation data using Seaborn\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(val_cm, annot=True, fmt='g', cmap='Blues', xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Validation Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print classification report for validation data\n",
        "print(\"\\nValidation Classification Report:\")\n",
        "print(classification_report(val_labels, val_predictions, target_names=['0', '1']))\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "# Flatten the predictions to 1D array for comparison\n",
        "test_predictions = test_predictions.flatten()  # Convert from (n_samples, 1) to (n_samples,)\n",
        "\n",
        "# Convert predictions to binary labels (True or False) based on threshold 0.5\n",
        "test_predictions = (test_predictions > 0.5)\n",
        "\n",
        "# Calculate accuracy for test dataset\n",
        "test_acc = np.mean(test_predictions == test_labels)\n",
        "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Calculate the confusion matrix for test data\n",
        "test_cm = confusion_matrix(test_labels, test_predictions)\n",
        "\n",
        "# Plot the confusion matrix for test data using Seaborn\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(test_cm, annot=True, fmt='g', cmap='Blues', xticklabels=['0', '1'], yticklabels=['0', '1'])\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Test Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print classification report for test data\n",
        "print(\"\\nTest Classification Report:\")\n",
        "print(classification_report(test_labels, test_predictions, target_names=['0', '1']))"
      ],
      "metadata": {
        "id": "cxF89aeETeU_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}